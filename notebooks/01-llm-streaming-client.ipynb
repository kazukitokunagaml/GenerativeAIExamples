{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea8890e",
   "metadata": {},
   "source": [
    "# LLM Streaming Client\n",
    "\n",
    "This notebook demonstrates how to stream responses from the LLM. \n",
    "\n",
    "### Triton Inference Server\n",
    "The LLM has been deployed to [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server) and leverages NVIDIA TensorRT-LLM (TRT-LLM), so it's optimized for low latency and high throughput inference.\n",
    "\n",
    "The Triton client is used to communicate with the inference server hosting the LLM and is available in [LangChain](https://github.com/langchain-ai/langchain-nvidia/tree/main/libs/trt). \n",
    "\n",
    "### Streaming LLM Responses\n",
    "TRT-LLM on its own can provide drastic improvements to LLM response latency, but streaming can take the user-experience to the next level. Instead of waiting for an entire response to be returned from the LLM, chunks of it can be processed as soon as they are available. This helps reduce the perceived latency by the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397d36f",
   "metadata": {},
   "source": [
    "### Step 1: Structure the Query in a Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec300ab0",
   "metadata": {},
   "source": [
    "A [**prompt template**](https://gpt-index.readthedocs.io/en/stable/api_reference/prompts.html) is a common paradigm in LLM development. \n",
    "\n",
    "They are a pre-defined set of instructions provided to the LLM and guide the output produced by the model. They can contain few shot examples and guidance and are a quick way to engineer the responses from the LLM. Llama 2 accepts the [prompt format](https://huggingface.co/blog/llama2#how-to-prompt-llama-2) shown in `LLAMA_PROMPT_TEMPLATE`, which we modify to be constructed with:\n",
    "- The system prompt\n",
    "- The context\n",
    "- The user's question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a929b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_PROMPT_TEMPLATE = (\n",
    " \"<s>[INST] <<SYS>>\"\n",
    " \"{system_prompt}\"\n",
    " \"<</SYS>>\"\n",
    " \"[/INST] {context} </s><s>[INST] {question} [/INST]\"\n",
    ")\n",
    "system_prompt = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are positive in nature.\"\n",
    "context=\"\"\n",
    "question='What is the fastest land animal?'\n",
    "prompt = LLAMA_PROMPT_TEMPLATE.format(system_prompt=system_prompt, context=context, question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c355feed",
   "metadata": {},
   "source": [
    "### Step 2: Create the Triton Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9ca50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WARNING!</b> Be sure to replace `triton_url` with the address and port that Triton is running on. \n",
    "</div>\n",
    "\n",
    "Use the address and port that the Triton is available on; for example `localhost:8001`. \n",
    "\n",
    "**If you are running this notebook as part of the AI workflow, you dont have to replace the url**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4abeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tritonclient/grpc/service_pb2_grpc.py:21: RuntimeWarning: The grpc package installed is at version 1.58.0, but the generated code in grpc_service_pb2_grpc.py depends on grpcio>=1.64.0. Please upgrade your grpc module to grpcio>=1.64.0 or downgrade your generated code using grpcio-tools<=1.58.0. This warning will become an error in 1.65.0, scheduled for release on June 25, 2024.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for TritonTensorRTLLM\n__root__\n  Channel.unary_unary() got an unexpected keyword argument '_registered_method' (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m triton_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm:8001\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m pload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserver_url\u001b[39m\u001b[38;5;124m'\u001b[39m: triton_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mTritonTensorRTLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpload\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for TritonTensorRTLLM\n__root__\n  Channel.unary_unary() got an unexpected keyword argument '_registered_method' (type=type_error)"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_trt.llms import TritonTensorRTLLM\n",
    "\n",
    "triton_url = \"llm:8001\"\n",
    "pload = {\n",
    "            'tokens':300,\n",
    "            'server_url': triton_url,\n",
    "            'model_name': \"ensemble\",\n",
    "            'temperature':1.0,\n",
    "            'top_k':1,\n",
    "            'top_p':0,\n",
    "            'beam_width':1,\n",
    "            'repetition_penalty':1.0,\n",
    "            'length_penalty':1.0\n",
    "}\n",
    "client = TritonTensorRTLLM(**pload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12edd642",
   "metadata": {},
   "source": [
    "Additional inputs to the LLM can be modified:\n",
    "- tokens: the maximum number of tokens (words/sub-words) generated\n",
    "- temperature: [0,1] -- higher values produce more diverse outputs\n",
    "- [top_k](https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p): sample from the k most likely next tokens at each step; lower value will concentrate sampling on the highest probability tokens for each step (reduces variety)\n",
    "- [top_p](https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p): [0, 1] -- cumulative probability cutoff for token selection; lower values mean sampling from a smaller nucleus sample (reduces variety)\n",
    "- repetition_penalty: [1, 2] -- penalize repeated tokens\n",
    "- length_penalty: 1 means no penalty for length of generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f1a45",
   "metadata": {},
   "source": [
    "### Step 3: Load the Model and Stream Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc922740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "start_time = time.time()\n",
    "tokens_generated = 0\n",
    "\n",
    "for val in client.stream(prompt):\n",
    "    tokens_generated += 1\n",
    "    print(val, end=\"\", flush=True)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n--- Generated {tokens_generated} tokens in {total_time} seconds ---\")\n",
    "print(f\"--- {tokens_generated/total_time} tokens/sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
