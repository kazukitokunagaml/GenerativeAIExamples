{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b360e33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 08:28:06.760 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "st.set_page_config(layout = \"wide\")\n",
    "\n",
    "with st.sidebar:\n",
    "    DOCS_DIR = os.path.abspath(\"./uploaded_docs\")\n",
    "    if not os.path.exists(DOCS_DIR):\n",
    "        os.makedirs(DOCS_DIR)\n",
    "    st.subheader(\"Add to the Knowledge Base\")\n",
    "    with st.form(\"my-form\", clear_on_submit=True):\n",
    "        uploaded_files = st.file_uploader(\"Upload a file to the Knowledge Base:\", accept_multiple_files = True)\n",
    "        submitted = st.form_submit_button(\"Upload!\")\n",
    "\n",
    "    if uploaded_files and submitted:\n",
    "        for uploaded_file in uploaded_files:\n",
    "            st.success(f\"File {uploaded_file.name} uploaded successfully!\")\n",
    "            with open(os.path.join(DOCS_DIR, uploaded_file.name),\"wb\") as f:\n",
    "                f.write(uploaded_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6f163ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンポーネント #1 - ドキュメントローダー\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# make sure to export your NVIDIA AI Playground key as NVIDIA_API_KEY!\n",
    "llm = ChatNVIDIA(model=\"mixtral_8x7b\")\n",
    "document_embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=\"passage\")\n",
    "query_embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2dca9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンポーネント#2 - エンベディング・モデルとLLM\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "import pickle\n",
    "\n",
    "with st.sidebar:\n",
    "    # Option for using an existing vector store\n",
    "    use_existing_vector_store = st.radio(\"Use existing vector store if available\", [\"Yes\", \"No\"], horizontal=True)\n",
    "\n",
    "# Path to the vector store file\n",
    "vector_store_path = \"vectorstore.pkl\"\n",
    "\n",
    "# Load raw documents from the directory\n",
    "raw_documents = DirectoryLoader(DOCS_DIR).load()\n",
    "\n",
    "\n",
    "# Check for existing vector store file\n",
    "vector_store_exists = os.path.exists(vector_store_path)\n",
    "vectorstore = None\n",
    "if use_existing_vector_store == \"Yes\" and vector_store_exists:\n",
    "    with open(vector_store_path, \"rb\") as f:\n",
    "        vectorstore = pickle.load(f)\n",
    "    with st.sidebar:\n",
    "        st.success(\"Existing vector store loaded successfully.\")\n",
    "else:\n",
    "    with st.sidebar:\n",
    "        if raw_documents:\n",
    "            with st.spinner(\"Splitting documents into chunks...\"):\n",
    "                text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "                documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "            with st.spinner(\"Adding document chunks to vector database...\"):\n",
    "                vectorstore = FAISS.from_documents(documents, document_embedder)\n",
    "\n",
    "            with st.spinner(\"Saving vector store\"):\n",
    "                with open(vector_store_path, \"wb\") as f:\n",
    "                    pickle.dump(vectorstore, f)\n",
    "            st.success(\"Vector store created and saved.\")\n",
    "        else:\n",
    "            st.warning(\"No documents available to process!\", icon=\"⚠️\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd85eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンポーネント#3 - ベクターデータベースストア\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "import pickle\n",
    "\n",
    "with st.sidebar:\n",
    "    # Option for using an existing vector store\n",
    "    use_existing_vector_store = st.radio(\"Use existing vector store if available\", [\"Yes\", \"No\"], horizontal=True)\n",
    "\n",
    "# Path to the vector store file\n",
    "vector_store_path = \"vectorstore.pkl\"\n",
    "\n",
    "# Load raw documents from the directory\n",
    "raw_documents = DirectoryLoader(DOCS_DIR).load()\n",
    "\n",
    "\n",
    "# Check for existing vector store file\n",
    "vector_store_exists = os.path.exists(vector_store_path)\n",
    "vectorstore = None\n",
    "if use_existing_vector_store == \"Yes\" and vector_store_exists:\n",
    "    with open(vector_store_path, \"rb\") as f:\n",
    "        vectorstore = pickle.load(f)\n",
    "    with st.sidebar:\n",
    "        st.success(\"Existing vector store loaded successfully.\")\n",
    "else:\n",
    "    with st.sidebar:\n",
    "        if raw_documents:\n",
    "            with st.spinner(\"Splitting documents into chunks...\"):\n",
    "                text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "                documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "            with st.spinner(\"Adding document chunks to vector database...\"):\n",
    "                vectorstore = FAISS.from_documents(documents, document_embedder)\n",
    "\n",
    "            with st.spinner(\"Saving vector store\"):\n",
    "                with open(vector_store_path, \"wb\") as f:\n",
    "                    pickle.dump(vectorstore, f)\n",
    "            st.success(\"Vector store created and saved.\")\n",
    "        else:\n",
    "            st.warning(\"No documents available to process!\", icon=\"⚠️\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f128fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 08:31:43.575 Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "st.session_state has no attribute \"messages\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state.py:398\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state.py:443\u001b[0m, in \u001b[0;36mSessionState._getitem\u001b[0;34m(self, widget_id, user_key)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# We'll never get here\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state_proxy.py:119\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state_proxy.py:90\u001b[0m, in \u001b[0;36mSessionStateProxy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     89\u001b[0m require_valid_user_key(key)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_session_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/safe_session_state.py:91\u001b[0m, in \u001b[0;36mSafeSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state.py:400\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(_missing_key_error_message(key))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'st.session_state has no key \"messages\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m st\u001b[38;5;241m.\u001b[39msession_state:\n\u001b[1;32m      8\u001b[0m     st\u001b[38;5;241m.\u001b[39msession_state\u001b[38;5;241m.\u001b[39mmessages \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m st\u001b[38;5;241m.\u001b[39mchat_message(message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m     12\u001b[0m         st\u001b[38;5;241m.\u001b[39mmarkdown(message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state_proxy.py:121\u001b[0m, in \u001b[0;36mSessionStateProxy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(_missing_attr_error_message(key))\n",
      "\u001b[0;31mAttributeError\u001b[0m: st.session_state has no attribute \"messages\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Component #4 - LLM Response Generation and Chat\n",
    "############################################\n",
    "\n",
    "st.subheader(\"Chat with your AI Assistant, Envie!\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a helpful AI assistant named Envie. You will reply to questions only based on the context that you are provided. If something is out of context, you will refrain from replying and politely decline to respond to the user.\"), (\"user\", \"{input}\")]\n",
    ")\n",
    "user_input = st.chat_input(\"Can you tell me what NVIDIA is known for?\")\n",
    "llm = ChatNVIDIA(model=\"mixtral_8x7b\")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "if user_input and vectorstore!=None:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    docs = retriever.get_relevant_documents(user_input)\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_input)\n",
    "\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += doc.page_content + \"\\n\\n\"\n",
    "\n",
    "    augmented_user_input = \"Context: \" + context + \"\\n\\nQuestion: \" + user_input + \"\\n\"\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        full_response = \"\"\n",
    "\n",
    "        for response in chain.stream({\"input\": augmented_user_input}):\n",
    "            full_response += response\n",
    "            message_placeholder.markdown(full_response + \"▌\")\n",
    "        message_placeholder.markdown(full_response)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ffd4e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error loading file /kqi/git/uploaded_docs/livedoor.pkl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid file /kqi/git/uploaded_docs/livedoor.pkl. The FileType.UNK file type is not supported in partition.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m vector_store_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlivedoor.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# ドキュメントの読み込み\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m raw_documents \u001b[38;5;241m=\u001b[39m \u001b[43mDirectoryLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDOCS_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 既存のベクトルストアを使用するかどうかの選択\u001b[39;00m\n\u001b[1;32m     31\u001b[0m use_existing_vector_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# ここを\"Yes\"または\"No\"に設定してください\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/directory.py:194\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[0;32m--> 194\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[1;32m    197\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/directory.py:137\u001b[0m, in \u001b[0;36mDirectoryLoader.load_file\u001b[0;34m(self, item, path, docs, pbar)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pbar:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/directory.py:130\u001b[0m, in \u001b[0;36mDirectoryLoader.load_file\u001b[0;34m(self, item, path, docs, pbar)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m     sub_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     docs\u001b[38;5;241m.\u001b[39mextend(sub_docs)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/unstructured.py:87\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements(elements)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/unstructured.py:179\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/partition/auto.py:490\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, skip_infer_table_types, ssl_verify, ocr_languages, languages, detect_language_per_element, pdf_infer_table_structure, pdf_extract_images, pdf_image_output_dir_path, xml_keep_tags, data_source_metadata, metadata_filename, request_timeout, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiletype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file type is not supported in partition.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements:\n\u001b[1;32m    493\u001b[0m     element\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m=\u001b[39m url\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid file /kqi/git/uploaded_docs/livedoor.pkl. The FileType.UNK file type is not supported in partition."
     ]
    }
   ],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import os\n",
    "import pickle\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# コンフィグレーション\n",
    "DOCS_DIR = os.path.abspath(\"./uploaded_docs\")\n",
    "if not os.path.exists(DOCS_DIR):\n",
    "    os.makedirs(DOCS_DIR)\n",
    "\n",
    "# ドキュメントのアップロード (Jupyterでは手動でファイルをアップロードしてディレクトリに配置してください)\n",
    "# Upload files to DOCS_DIR manually or use appropriate Jupyter notebook upload widget\n",
    "\n",
    "# Embedding Model and LLMの設定\n",
    "llm = ChatNVIDIA(model=\"mixtral_8x7b\")\n",
    "document_embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=\"passage\")\n",
    "query_embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=\"query\")\n",
    "\n",
    "# Vector Database Storeの設定\n",
    "vector_store_path = \"livedoor.pkl\"\n",
    "\n",
    "# ドキュメントの読み込み\n",
    "raw_documents = DirectoryLoader(DOCS_DIR).load()\n",
    "\n",
    "# 既存のベクトルストアを使用するかどうかの選択\n",
    "use_existing_vector_store = \"Yes\"  # ここを\"Yes\"または\"No\"に設定してください\n",
    "\n",
    "vector_store_exists = os.path.exists(vector_store_path)\n",
    "vectorstore = None\n",
    "if use_existing_vector_store == \"Yes\" and vector_store_exists:\n",
    "    with open(vector_store_path, \"rb\") as f:\n",
    "        vectorstore = pickle.load(f)\n",
    "    print(\"既存のベクトルストアを正常にロードしました。\")\n",
    "else:\n",
    "    if raw_documents:\n",
    "        print(\"ドキュメントをチャンクに分割しています...\")\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "        documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "        print(\"ドキュメントチャンクをベクトルデータベースに追加しています...\")\n",
    "        vectorstore = FAISS.from_documents(documents, document_embedder)\n",
    "\n",
    "        print(\"ベクトルストアを保存しています...\")\n",
    "        with open(vector_store_path, \"wb\") as f:\n",
    "            pickle.dump(vectorstore, f)\n",
    "        print(\"ベクトルストアが作成され保存されました。\")\n",
    "    else:\n",
    "        print(\"処理可能なドキュメントがありません！\")\n",
    "\n",
    "# LLM応答生成とチャット\n",
    "def chat_with_envie(user_input):\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", \"You are a helpful AI assistant named Envie. You will reply to questions only based on the context that you are provided. If something is out of context, you will refrain from replying and politely decline to respond to the user.\"), (\"user\", \"{input}\")]\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    if user_input and vectorstore != None:\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        docs = retriever.get_relevant_documents(user_input)\n",
    "\n",
    "        context = \"\"\n",
    "        for doc in docs:\n",
    "            context += doc.page_content + \"\\n\\n\"\n",
    "\n",
    "        augmented_user_input = \"Context: \" + context + \"\\n\\nQuestion: \" + user_input + \"\\n\"\n",
    "\n",
    "        full_response = \"\"\n",
    "\n",
    "        for response in chain.stream({\"input\": augmented_user_input}):\n",
    "            full_response += response\n",
    "        return full_response\n",
    "\n",
    "# ユーザーからの入力を取得\n",
    "user_input = \"Can you tell me what NVIDIA is known for?\"\n",
    "\n",
    "# チャットを実行\n",
    "response = chat_with_envie(user_input)\n",
    "print(\"Envie:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5215f95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonlファイルをpickleファイルに変換しました。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# jsonlファイルのパス\n",
    "jsonl_file_path = './uploaded_docs/livedoor.jsonl'\n",
    "\n",
    "# pickleファイルのパス\n",
    "pickle_file_path = 'livedoor.pkl'\n",
    "\n",
    "# jsonlファイルの読み込み\n",
    "data = []\n",
    "with open(jsonl_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# pickleファイルに保存\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(\"jsonlファイルをpickleファイルに変換しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c56c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
